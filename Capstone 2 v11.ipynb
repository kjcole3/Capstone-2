{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "__Problem:__   Companies today are creating petabytes of new data, mostly electronic, each year. This has become a cost and logistics problem for many corporations because most do not have an enforceable document management policy for electronic data. This is not because of negligence, but rather ineffective tools on how to govern and regulate the knowledge that is shared electronically today. \n",
    "\n",
    "One idea to solve this problem is to create a document classification engine that will assign “categories” or “tags” to a document regardless of document location or filename. A tool like this would give flexibility to the end user, but also the governing power to the records manager so that they can apply best practices from physical records management\n",
    "\n",
    "__Goal:__   In this notebook we will narrow our focus to building a model that will classify documents into one of three categories: Operations, Legal and Accounting. The notebook will test different combinations of text transfomers and classifiers and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Adding Required Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing work\n",
    "Loop through each text files acquired from OCR and 'preprocess' the data by removing stop words, extra spaces, and reducing the words to their root, or stem. We will then zip the contents into a Pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"ANSI\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield content\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training set is pre-classified according to subfolder\n",
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Ops'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    # tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    # Remove stop words known in nltk package\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    # Use word stemmeing to get root meaning\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    # Concatenate words into one string and then add to list_\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "ops_df = pd.DataFrame(data = list_)\n",
    "ops_df['class']=\"ops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Legal'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "legal_df = pd.DataFrame(data = list_)\n",
    "legal_df['class']=\"legal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Accounting'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "accounting_df = pd.DataFrame(data = list_)\n",
    "accounting_df['class']=\"accounting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble training and test data\n",
    "We will use a 50/50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = ops_df.append(legal_df).append(accounting_df)\n",
    "merged_df.columns = ['text', 'cat']\n",
    "merged_df = merged_df.sort_index().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 1113 total training examples:\n",
    "* 580 Ops\n",
    "* 209 Legal\n",
    "* 324 Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will flatten the data into (sample, feature) matrices\n",
    "X = merged_df.text\n",
    "y = merged_df.cat\n",
    "\n",
    "# and then split the dataset into two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting\n",
    "To streamline the code, I will be using both pipelines and grid_search in the model fitting steps. Pipelines allow for a linear sequence of data transforms to be chained together in a modeling process that can be evaluated. The purpose of the grid_search is to evaluate the model under different combinations of hyper-parameters. This will result in single 'best estimator'.\n",
    "\n",
    "There will be 3 models evaluated:\n",
    "* Model 1: CountVectorizer, TfidfTransformer, and SGDCLassifier\n",
    "* Model 2: HashVectorizer and SGDClassifier\n",
    "* Model 3: TfidfVectorizer and LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model 1\n",
    "Here are some of the model features seen in this section:\n",
    "* CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "* TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation. \n",
    "* SGDClassifier (Stochastic Gradient Decent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the pipeline steps:\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters...\n",
      "\n",
      "Default hyper-parameters of classifier:\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=1000, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters...\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters)\n",
    "clf.fit(X = X_train, y = y_train)\n",
    "print(\"Default hyper-parameters of classifier:\")\n",
    "print(pipeline.steps[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned-parameters set found on development set:\n",
      "{'clf__alpha': 1e-06, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'vect__max_df': 1.0}\n",
      "\n",
      "Best score on development set:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Best tuned-parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       170\n",
      "      legal       1.00      1.00      1.00       100\n",
      "        ops       1.00      1.00      1.00       287\n",
      "\n",
      "avg / total       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "Detailed classification report on training set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       154\n",
      "      legal       1.00      1.00      1.00       109\n",
      "        ops       1.00      1.00      1.00       293\n",
      "\n",
      "avg / total       1.00      1.00      1.00       556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report on training set\")\n",
    "print()\n",
    "y_true, y_pred = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis of Model 1\n",
    "As seen in the detailed classification report, both the precision and recall of the model are near 100%,\n",
    "and therefore the harmonic mean (f-score) is also 100%.This indicates a near perfect model fit to the dataset. \n",
    "\n",
    "Precision is the number of true positives over the number of true positives plus false positives. \n",
    "Recall is the number of true positives over the number of true positives plus false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "In Model 2 a different vectorizer, HashingVectorizer, is used in the pipeline in order to evaluate the effect CountVectorizer had on Model 1. The HashingVectorizer will convert the text documents to a matrix of token occurrences by using a hashing trick to find the token string name to feature integer index mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the pipeline steps:\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', HashingVectorizer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__norm': ('l1','l2',None),\n",
    "    'vect__alternate_sign': [0,1],\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters...\n",
      "\n",
      "Default hyper-parameters of classifier:\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=1000, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters...\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters)\n",
    "clf.fit(X = X_train, y = y_train)\n",
    "print(\"Default hyper-parameters of classifier:\")\n",
    "print(pipeline.steps[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__alpha': 1e-05, 'clf__penalty': 'elasticnet', 'vect__alternate_sign': 0, 'vect__norm': 'l2'}\n",
      "\n",
      "Best score on development set:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      0.99      1.00       170\n",
      "      legal       0.99      1.00      1.00       100\n",
      "        ops       1.00      1.00      1.00       287\n",
      "\n",
      "avg / total       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "Detailed classification report on training set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       154\n",
      "      legal       1.00      1.00      1.00       109\n",
      "        ops       1.00      1.00      1.00       293\n",
      "\n",
      "avg / total       1.00      1.00      1.00       556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report on training set\")\n",
    "print()\n",
    "y_true, y_pred = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Model 2\n",
    "Model 2 score results are nearly identical to Model 1. There will need to be further analysis before we can confidently declare which vectorizer is a better fit to the model. Fortunately, both have shown good results. \n",
    "\n",
    "One benfit of using the HashingVectorizer is that it scales very well since it does not have memory. However, \"there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\" (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "In Model 3 I will apply a new classifier, Support Vector Machine (SVM) in order to evaluate the effectivness of the SGD classifier on Model 1. SVM is one of the oldest AI algorithms, and is the basis for neural networks. SVC does not care about the 'perfect' point, instead it wants the 'ugliest' point that still classifies. We will tune the slack variable (c) in our model. \n",
    "\n",
    "In addition, I will resume using the same transformers from Model 1, CountVectorizer and TfidfTransformer, which are combined into a TfidfVectorizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the pipeline steps\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = dict(tfidf__sublinear_tf=[0,1],\n",
    "                  tfidf__smooth_idf=[0,1],\n",
    "                  tfidf__norm =['l1','l2',None],\n",
    "                  clf__C=[1, 10, 100]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters...\n",
      "\n",
      "Default hyper-parameters of classifier:\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters...\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters)\n",
    "clf.fit(X = X_train, y = y_train)\n",
    "print(\"Default hyper-parameters of classifier:\")\n",
    "print(pipeline.steps[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__C': 10, 'tfidf__norm': 'l1', 'tfidf__smooth_idf': 0, 'tfidf__sublinear_tf': 0}\n",
      "\n",
      "Best score on development set:\n",
      "0.996402877698\n"
     ]
    }
   ],
   "source": [
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      0.99      0.99       170\n",
      "      legal       0.97      1.00      0.99       100\n",
      "        ops       1.00      1.00      1.00       287\n",
      "\n",
      "avg / total       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Detailed classification report on training set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      0.99      1.00       154\n",
      "      legal       0.99      1.00      1.00       109\n",
      "        ops       1.00      1.00      1.00       293\n",
      "\n",
      "avg / total       1.00      1.00      1.00       556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report on training set\")\n",
    "print()\n",
    "y_true, y_pred = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis of Model 3\n",
    "The SVM classifier is performing well, but the best score is slightly less lower than the SGD classifier. Precision, Recall and the combined F1 score are all near 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis of Results\n",
    "It is very curious that we have seen such good statistical results on all three models. To analyze the results further I will refit the data to Model 1, and then look for insights into the words used ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__alpha': 1e-05, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__max_df': 0.5}\n",
      "\n",
      "Best score on development set:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline steps:\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(pipeline, parameters)\n",
    "gsearch.fit(X = X_train, y = y_train)\n",
    "\n",
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(gsearch.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(gsearch.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are ,  17498  features in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are ,\", len(gsearch.best_estimator_.steps[0][1].vocabulary_), \" features in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature weighting\n",
    "Which features are being assigned higher a weight/significance, and which have the least effect on the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features with highest significance: \n",
      "(-8.5020729274696833, 'iiii')\n",
      "(-7.1207111824127214, 'pul')\n",
      "(-6.6695182824853596, 'scan')\n",
      "(-6.3876111606147257, '0620693')\n",
      "(-6.1254828232685758, '45')\n",
      "(-5.8944011924991182, '2407')\n",
      "(-5.6945778517060219, '2017')\n",
      "(-5.6623854031324008, '0621483')\n",
      "(-5.6260594416621617, 'sit')\n",
      "(-5.5672657359389479, 'boat')\n",
      "(-5.5198551298360377, '0331534_20120430')\n",
      "(-5.5198551298360377, '25am')\n",
      "(-5.3769527315334908, 'flat')\n",
      "(-5.3248471864722902, 'agr')\n",
      "(-5.1428324665087182, '154')\n",
      "(-5.1379049049677494, '0621441')\n",
      "(-5.1169338892601059, '02rcvgtm0121')\n",
      "(-5.0317786957614903, '02rcvgbx0189')\n",
      "(-4.9277476596731677, 'vault')\n",
      "(-4.6085086552407937, 'tap')\n",
      "\n",
      " \n",
      "\n",
      "20 features with lowest significance: \n",
      "(7.4577434283106587, '2016')\n",
      "(7.2613598223493812, '99')\n",
      "(7.1799707325021975, '390')\n",
      "(7.1436530909851719, 'serv')\n",
      "(6.6706837680264739, '250')\n",
      "(6.616028843130354, 'invo')\n",
      "(6.4355836706132088, '000')\n",
      "(6.3746651635444316, '16')\n",
      "(6.2062799714373522, '36')\n",
      "(6.0827398743368999, '37')\n",
      "(6.006091274994799, 'amount')\n",
      "(5.6786771064096433, 'policy')\n",
      "(5.1573414102946522, '049')\n",
      "(4.8353955930913237, 'refrig')\n",
      "(4.7632023062088757, 'ins')\n",
      "(4.5090683487114118, 'tgs')\n",
      "(4.5090683487114118, 'langlo')\n",
      "(4.4659790991317907, 'tn')\n",
      "(4.4104257057096206, 'tag')\n",
      "(4.3473095623913407, '72')\n"
     ]
    }
   ],
   "source": [
    "feature_names = gsearch.best_estimator_.steps[0][1].get_feature_names()\n",
    "coefs_with_fns = sorted(zip(gsearch.best_estimator_.steps[-1][1].coef_[0], feature_names))\n",
    "bestFeat = coefs_with_fns[:20]\n",
    "worstFeat = coefs_with_fns[:-(20 + 1):-1]\n",
    "print(\"20 features with highest significance: \")\n",
    "for feat in bestFeat:\n",
    "    print(feat)\n",
    "print('\\n','\\n')\n",
    "print(\"20 features with lowest significance: \")\n",
    "for feat in worstFeat:\n",
    "    print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most repeated features in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 most repeated word stems in decending frequency:\n",
      "[('stor', 2385), ('agr', 2308), ('shal', 1918), ('midcon', 1895), ('company', 1817), ('serv', 1766), ('mat', 1174), ('16', 1154), ('charg', 1120), ('provid', 997), ('11', 977), ('work', 959), ('2016', 947), ('fil', 896), ('pay', 888), ('us', 887), ('invo', 856), ('10', 847), ('delivery', 799), ('405', 792)]\n"
     ]
    }
   ],
   "source": [
    "matrix = gsearch.best_estimator_.steps[0][1].fit_transform(X_train, y_train)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in gsearch.best_estimator_.steps[0][1].vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "sort_freq = sorted (freqs, key = lambda x: -x[1])\n",
    "#print top 20\n",
    "print(\"The 20 most repeated word stems in decending frequency:\")\n",
    "print(sort_freq[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test/Train split\n",
    "As a final test, I will demonstrate that the test train split does not have an effect on the model score by varying the test/train split to:\n",
    "* 60/40\n",
    "* 40/60\n",
    "* 30/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__alpha': 1e-05, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__max_df': 0.5}\n",
      "\n",
      "Best score on development set:\n",
      "0.997752808989\n",
      "\n",
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       202\n",
      "      legal       0.98      1.00      0.99       118\n",
      "        ops       1.00      1.00      1.00       348\n",
      "\n",
      "avg / total       1.00      1.00      1.00       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0)\n",
    "\n",
    "# Define the pipeline steps:\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(pipeline, parameters)\n",
    "gsearch.fit(X = X_train, y = y_train)\n",
    "\n",
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(gsearch.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(gsearch.best_score_)\n",
    "print()\n",
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, gsearch.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__alpha': 1e-05, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__max_df': 0.75}\n",
      "\n",
      "Best score on development set:\n",
      "1.0\n",
      "\n",
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       145\n",
      "      legal       1.00      1.00      1.00        70\n",
      "        ops       1.00      1.00      1.00       231\n",
      "\n",
      "avg / total       1.00      1.00      1.00       446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# Define the pipeline steps:\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(pipeline, parameters)\n",
    "gsearch.fit(X = X_train, y = y_train)\n",
    "\n",
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(gsearch.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(gsearch.best_score_)\n",
    "print()\n",
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, gsearch.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned parameters set found on development set:\n",
      "{'clf__alpha': 1e-05, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__max_df': 0.75}\n",
      "\n",
      "Best score on development set:\n",
      "1.0\n",
      "\n",
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       106\n",
      "      legal       1.00      1.00      1.00        51\n",
      "        ops       1.00      1.00      1.00       177\n",
      "\n",
      "avg / total       1.00      1.00      1.00       334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Define the pipeline steps:\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=.0001)),\n",
    "])\n",
    "\n",
    "# hyper-parameters to be tuned:\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(pipeline, parameters)\n",
    "gsearch.fit(X = X_train, y = y_train)\n",
    "\n",
    "print(\"Best tuned parameters set found on development set:\")\n",
    "print(gsearch.best_params_)\n",
    "print()\n",
    "print(\"Best score on development set:\")\n",
    "print(gsearch.best_score_)\n",
    "print()\n",
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, gsearch.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
