{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Set-up\n",
    "\n",
    "A demonstration of Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"ANSI\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield content\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing work\n",
    "Loop through each text files acquired from OCR and scrub the data\n",
    "\n",
    "We will then zip the list into a DataFrame.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training set is pre-classified according to subfolder subfolder\n",
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Ops'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    # tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    # Remove stop words known in nltk package\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    # Use word stemmeing to get root meaning\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    # Concatenate words into one string and then add to list_\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "ops_df = pd.DataFrame(data = list_)\n",
    "ops_df['class']=\"ops\"\n",
    "len(ops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Legal'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "legal_df = pd.DataFrame(data = list_)\n",
    "legal_df['class']=\"legal\"\n",
    "len(legal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r'C:\\Users\\osutr_000\\Documents\\Data\\Accounting'\n",
    "list_ = []\n",
    "\n",
    "for text in read_files(path):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    stop = stopwords.words('english')\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    lanste = LancasterStemmer()\n",
    "    intermediate = [lanste.stem(i) for i in intermediate]\n",
    "    final = \" \".join(intermediate)\n",
    "    list_.append(final)\n",
    "accounting_df = pd.DataFrame(data = list_)\n",
    "accounting_df['class']=\"accounting\"\n",
    "len(accounting_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = ops_df.append(legal_df).append(accounting_df)\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are 1113 total training examples:\n",
    "# 580 Ops\n",
    "# 209 Legal\n",
    "# 324 Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>401 w 33rd edmond ok 73013 www midcondat com 4...</td>\n",
       "      <td>ops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>26938 002 4 27 16 290 60 290 60 5 4 16 26938 d...</td>\n",
       "      <td>accounting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>midcon dat serv llc thi dat destruct agr mad e...</td>\n",
       "      <td>legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5 25 16 179 17 179 17 5 27 16 01605 05 republ ...</td>\n",
       "      <td>accounting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>legal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text         cat\n",
       "0      0  401 w 33rd edmond ok 73013 www midcondat com 4...         ops\n",
       "1      0  26938 002 4 27 16 290 60 290 60 5 4 16 26938 d...  accounting\n",
       "2      0  midcon dat serv llc thi dat destruct agr mad e...       legal\n",
       "3      1  5 25 16 179 17 179 17 5 27 16 01605 05 republ ...  accounting\n",
       "4      1                                                          legal"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns = ['text', 'cat']\n",
    "merged_df.sort_index().reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can see how the processed data is now structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will flatten the data into (sample, feature) matrices\n",
    "X = merged_df.text\n",
    "y = merged_df.cat\n",
    "\n",
    "# and then split the dataset into two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model 1\n",
    "Here are some of the model features seen in this section:\n",
    "* CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "\n",
    "* TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation. \n",
    "The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus \n",
    "that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "* SGDCClassifier (Stochastic Gradient Decent) has gain popularity in NLP applications for it's scalability.\n",
    "SGD has a lot of parameters and is sensitive to feature scaling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a pipeline with a text feature extractor and classifier:\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# hyper-parameters\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.930 (+/-0.037) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "0.950 (+/-0.079) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 0.75}\n",
      "0.932 (+/-0.040) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 1.0}\n",
      "0.993 (+/-0.014) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 0.5}\n",
      "0.993 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 0.75}\n",
      "0.993 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 1.0}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "0.996 (+/-0.010) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.75}\n",
      "0.996 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 1.0}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 0.5}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 0.75}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 1.0}\n",
      "0.928 (+/-0.036) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "0.950 (+/-0.079) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 0.75}\n",
      "0.950 (+/-0.079) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__max_df': 1.0}\n",
      "0.993 (+/-0.014) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 0.5}\n",
      "0.993 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 0.75}\n",
      "0.995 (+/-0.000) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__max_df': 1.0}\n",
      "0.996 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.75}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 1.0}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 0.5}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 0.75}\n",
      "0.998 (+/-0.005) for {'clf__max_iter': 5, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__max_df': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters...\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       160\n",
      "      legal       0.99      1.00      1.00       106\n",
      "        ops       1.00      1.00      1.00       291\n",
      "\n",
      "avg / total       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "Detailed classification report on training set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       164\n",
      "      legal       1.00      1.00      1.00       103\n",
      "        ops       1.00      1.00      1.00       289\n",
      "\n",
      "avg / total       1.00      1.00      1.00       556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report on training set\")\n",
    "print()\n",
    "y_true, y_pred = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis of Model 1\n",
    "As seen in the detailed classification report, both the precision and recall of the model are near 100%,\n",
    "and therefore the harmonic mean (f-score) is also 100%.This indicates a good model fit to the dataset. \n",
    "\n",
    "Precision is the number of true positives over the number of true positives plus false positives. \n",
    "Recall is the number of true positives over the number of true positives plus false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "Here are some of the features you will see tried in this model:\n",
    "* TfidfVectorizer combines the features of the previously used packages - CountVectorizer and TfidfTransformer\n",
    "* Support Vector Classification (SVC) doesn't like high dimensionality (number of features) so I applied a demensionality reducer called TruncatedSVD.\n",
    "* SVM-C is one of the oldest AI algorithms, and is the basis for neural networks. SVC doesn't care about the 'perfect' point, rather it wasnts the 'ugliest' point that still classifies. You can control the kernel type and slack variable (c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "param_grid = dict(tfidf__sublinear_tf=[0,1],\n",
    "                  svd__n_components=[10, 20, 100],\n",
    "                  clf__kernel=['linear','rbf'],\n",
    "                  clf__C=[1, 10, 100]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters...\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.964 (+/-0.078) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.964 (+/-0.078) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.987 (+/-0.005) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.010) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.991 (+/-0.018) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.995 (+/-0.015) for {'clf__C': 1, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n",
      "0.930 (+/-0.030) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.930 (+/-0.030) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.924 (+/-0.031) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.917 (+/-0.021) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.520 (+/-0.003) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.520 (+/-0.003) for {'clf__C': 1, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n",
      "0.989 (+/-0.009) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.993 (+/-0.014) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.989 (+/-0.009) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.989 (+/-0.009) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.989 (+/-0.009) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.018) for {'clf__C': 10, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n",
      "0.987 (+/-0.005) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.010) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.987 (+/-0.005) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.010) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.932 (+/-0.032) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.932 (+/-0.032) for {'clf__C': 10, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n",
      "0.995 (+/-0.009) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.996 (+/-0.010) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.996 (+/-0.010) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.995 (+/-0.009) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.993 (+/-0.014) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.018) for {'clf__C': 100, 'clf__kernel': 'linear', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n",
      "0.991 (+/-0.010) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 0}\n",
      "0.993 (+/-0.014) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 10, 'tfidf__sublinear_tf': 1}\n",
      "0.989 (+/-0.009) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 0}\n",
      "0.989 (+/-0.009) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 20, 'tfidf__sublinear_tf': 1}\n",
      "0.989 (+/-0.009) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 0}\n",
      "0.991 (+/-0.018) for {'clf__C': 100, 'clf__kernel': 'rbf', 'svd__n_components': 100, 'tfidf__sublinear_tf': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters...\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on test set:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       160\n",
      "      legal       0.99      1.00      1.00       106\n",
      "        ops       1.00      1.00      1.00       291\n",
      "\n",
      "avg / total       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "Detailed classification report on training set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " accounting       1.00      1.00      1.00       164\n",
      "      legal       1.00      1.00      1.00       103\n",
      "        ops       1.00      1.00      1.00       289\n",
      "\n",
      "avg / total       1.00      1.00      1.00       556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report on test set:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report on training set\")\n",
    "print()\n",
    "y_true, y_pred = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis of Model 2\n",
    "Given that Gradient Decent and Support Vector Machines are of the same family, it is notsurpring Model 2 results are similar to Model 1. Precision, Recall and the combined F1 score are all averaging 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "I will explore a random forrest approach in Model 3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
